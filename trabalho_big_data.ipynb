{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1CtqKv4RSJS"
      },
      "source": [
        "# Comandos para realização do trabalho da matéria de Big Data com uso da biblioteca PySpark.\n",
        "\n",
        "## <font color=red>Observação importante:</font>\n",
        "\n",
        "<font color=yellow>Trabalho realizado com uso da biblioteca pandas não será aceito!</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbkK_VTuRSJV"
      },
      "source": [
        "## Upload do arquivo `imdb-reviews-pt-br.csv` para dentro do Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mEQCKe0wRSJW",
        "outputId": "f3581a97-9e9e-4b45-b1c0-8ac0eeab1a3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-25 19:17:46--  https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49549692 (47M) [application/zip]\n",
            "Saving to: ‘imdb-reviews-pt-br.zip’\n",
            "\n",
            "imdb-reviews-pt-br. 100%[===================>]  47.25M   168MB/s    in 0.3s    \n",
            "\n",
            "2024-10-25 19:17:47 (168 MB/s) - ‘imdb-reviews-pt-br.zip’ saved [49549692/49549692]\n",
            "\n",
            "Archive:  imdb-reviews-pt-br.zip\n",
            "replace imdb-reviews-pt-br.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip -O imdb-reviews-pt-br.zip\n",
        "!unzip imdb-reviews-pt-br.zip\n",
        "!rm imdb-reviews-pt-br.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snykJta7RSJX"
      },
      "source": [
        "## Instalação manual das dependências para uso do pyspark no Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pPNLluGFRSJX",
        "outputId": "82e03e31-e25d-43a5-c7d7-c71ed8da73d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlvNJSEdRSJX"
      },
      "source": [
        "## Importar, instanciar e criar a SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vWZgV2BhRSJY"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "appName = \"PySpark Trabalho de Big Data\"\n",
        "master = \"local\"\n",
        "\n",
        "spark = SparkSession.builder.appName(appName).master(master).getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiHZBeQfRSJY"
      },
      "source": [
        "## Criar spark dataframe do CSV utilizando o método read.csv do spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jUXoaYu1RSJY"
      },
      "outputs": [],
      "source": [
        "imdb_df = spark.read.csv('imdb-reviews-pt-br.csv',\n",
        "                         header=True,\n",
        "                         quote=\"\\\"\",\n",
        "                         escape=\"\\\"\",\n",
        "                         encoding=\"UTF-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAZ1FqO-RSJZ"
      },
      "source": [
        "# Questão 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrx5Yw2gRSJZ"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave e o \"id\" como valor do tipo inteiro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZFWEalyPRSJa"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "def map_sentiment_id(df):\n",
        "    # Converter a coluna \"id\" para inteiro\n",
        "    df = df.withColumn(\"id\", col(\"id\").cast(\"int\"))\n",
        "\n",
        "    # Criar um dicionário com \"sentimento\" como chave e listas de \"id\" como valores\n",
        "    sentiment_id_map = df.select(\"sentiment\", \"id\").rdd.map(lambda row: (row.sentiment, row.id)).groupByKey().mapValues(list).collectAsMap()\n",
        "\n",
        "    return sentiment_id_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6typqPMRSJa"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar os IDs por \"sentiment\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZY_tXabSRSJa"
      },
      "outputs": [],
      "source": [
        "# Função para reduzir e somar os IDs por \"sentiment\"\n",
        "def reduce_sum_ids(df):\n",
        "    # Converter a coluna \"id\" para inteiro\n",
        "    df = df.withColumn(\"id\", col(\"id\").cast(\"int\"))\n",
        "\n",
        "    # Converter para RDD e aplicar reduceByKey\n",
        "    sum_ids_rdd = df.select(\"sentiment\", \"id\").rdd.map(lambda row: (row.sentiment, row.id)).reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "    # Coletar e imprimir os resultados\n",
        "    result = sum_ids_rdd.collect()\n",
        "    for row in result:\n",
        "        print(f\"Sentiment: {row[0]}, Soma dos IDs: {row[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-dfQtcmRSJb"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "odr5y9TXRSJb",
        "outputId": "53e58f5a-dd33-4241-a2a8-35f3021275b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: neg, Soma dos IDs: 459568555\n",
            "Sentiment: pos, Soma dos IDs: 763600041\n"
          ]
        }
      ],
      "source": [
        "# Aplicar map/reduce e realizar collect\n",
        "sum_ids_rdd = imdb_df.select(\"sentiment\", \"id\").rdd.map(lambda row: (row.sentiment, int(row.id))).reduceByKey(lambda x, y: x + y).collect()\n",
        "\n",
        "# Visualizar os dados\n",
        "for row in sum_ids_rdd:\n",
        "    print(f\"Sentiment: {row[0]}, Soma dos IDs: {row[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79xxUJaKRSJb"
      },
      "source": [
        "# Questão 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoCmnKMuRSJb"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave e uma tupla com a soma das palavras de cada texto como valor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FgKrlnmQRSJb"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import split, size\n",
        "\n",
        "def map_sentiment_wordcount(df):\n",
        "    # Contar o número de palavras em cada texto (pt e en)\n",
        "    df = df.withColumn(\"text_pt_wordcount\", size(split(col(\"text_pt\"), \" \")))\n",
        "    df = df.withColumn(\"text_en_wordcount\", size(split(col(\"text_en\"), \" \")))\n",
        "\n",
        "    # Converter para RDD e mapear \"sentiment\" com a soma das palavras\n",
        "    sentiment_wordcount_rdd = df.select(\"sentiment\", \"text_pt_wordcount\", \"text_en_wordcount\")\\\n",
        "                                .rdd.map(lambda row: (row.sentiment, (row.text_pt_wordcount, row.text_en_wordcount)))\n",
        "\n",
        "    return sentiment_wordcount_rdd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwDkyMxIRSJc"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar o numero de palavras de cada texto português e inglês por \"sentiment\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Feg0oIuDRSJc"
      },
      "outputs": [],
      "source": [
        "def reduce_sum_wordcount(rdd):\n",
        "    # Aplicar reduceByKey para somar as contagens de palavras para cada sentimento\n",
        "    sum_wordcount_rdd = rdd.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
        "\n",
        "    # Coletar e imprimir os resultados\n",
        "    result = sum_wordcount_rdd.collect()\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWaDS2vfRSJc"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado\n",
        "\n",
        "1. Aplicar o map/reduce no seu dataframe spark e realizar o collect() ao final\n",
        "2. Selecionar os dados referentes aos textos negativos para realizar a subtração.\n",
        "3. Realizar a subtração das contagens de palavras dos textos negativos para obter o resultado final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_EnPFQknRSJc",
        "outputId": "4e7d2fe7-c9b8-4a24-e395-82eef814d7ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contagem das Palavras em Textos Negativos (PT - EN): 54036\n"
          ]
        }
      ],
      "source": [
        "# Chamando as funções map e reduce\n",
        "sentiment_wordcount_rdd = map_sentiment_wordcount(imdb_df)\n",
        "wordcount_results = reduce_sum_wordcount(sentiment_wordcount_rdd)\n",
        "\n",
        "# Selecionar dados dos textos negativos\n",
        "negative_wordcount = [item for item in wordcount_results if item[0] == 'neg'][0]\n",
        "\n",
        "# Realizar a subtração das contagens de palavras\n",
        "subtracted_wordcount = (negative_wordcount[1][0] - negative_wordcount[1][1])\n",
        "\n",
        "print(f\"Contagem das Palavras em Textos Negativos (PT - EN): {subtracted_wordcount}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}